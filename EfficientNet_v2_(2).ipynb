{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2Q-nVCrLPpg6"},"outputs":[],"source":["import os\n","import zipfile\n","from google.colab import files, drive\n","\n","# # Keggle API\n","# uploaded = files.upload()\n","\n","# # Move kaggle.json to .kaggle & Authorization\n","# !mkdir -p ~/.kaggle\n","# !mv kaggle.json ~/.kaggle/\n","\n","# !chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","source":["# Download Outside of Google Drive\n","# !kaggle datasets download -d dougandrade/dog-emotions-5-classes"],"metadata":{"id":"qiBPMwMcPtsk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"_jG62P9-ztcA"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"nRVgivUzPwAd","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"error","timestamp":1724467270535,"user_tz":-540,"elapsed":10,"user":{"displayName":"김이정","userId":"17258177918259810377"}},"outputId":"e53532b1-cc46-4853-9033-ddcddfd85012"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'drive' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3b8a479202a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"]}]},{"cell_type":"code","source":["# # Unzip\n","# with zipfile.ZipFile('/content/drive/MyDrive/train_images_5_class.zip', 'r') as zip_ref:\n","#     zip_ref.extractall(path='/content/drive/MyDrive/')"],"metadata":{"id":"Bxl09rJJQb7v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Module Import"],"metadata":{"id":"xidtdJ1h-2T2"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.models import EfficientNet_V2_S_Weights, efficientnet_v2_s\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data.dataset import random_split\n","from PIL import Image\n","from torchvision import datasets, models\n","from copy import deepcopy\n","import cv2\n","import glob\n","import argparse\n","import time\n","import json\n","import tensorflow as tf"],"metadata":{"id":"nzvqiDRXRtZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GPU Setting"],"metadata":{"id":"gvCJtz1r-9jg"}},{"cell_type":"code","source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPUs를 검색 및 해결\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","    print(\"Running on TPU\")\n","except ValueError:\n","    print(\"No TPU found, using default strategy\")\n","    strategy = tf.distribute.get_strategy()  # TPU가 없는 경우 기본 전략 사용 (예: CPU/GPU)"],"metadata":{"id":"ewljbdhTiDuv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724378289521,"user_tz":-540,"elapsed":13929,"user":{"displayName":"배고프다","userId":"10599614557067348512"}},"outputId":"f2d61f9b-8008-4c9c-e8fe-c2f86804c12e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on TPU\n"]}]},{"cell_type":"code","source":["# GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"GTzvyKWS9BT-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import Datasets"],"metadata":{"id":"jo45GEjHNsu8"}},{"cell_type":"code","source":["import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","\n","root_dir = '/content/drive/MyDrive/ToyProject'\n","\n","# 데이터셋 경로 설정\n","data_dir = f'{root_dir}/train_images_5_class'\n","\n","# EfficientNet v2에 맞는 전처리 파이프라인 설정\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),  # EfficientNet의 입력 크기에 맞게 조정\n","    transforms.ToTensor(),  # 이미지를 Tensor로 변환\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Pre-trained 모델에 맞는 정규화\n","])\n","\n","# ImageFolder를 사용하여 폴더 구조에서 데이터셋 로드\n","dataset = datasets.ImageFolder(root=data_dir, transform=preprocess)\n","\n","# 클래스 레이블 확인\n","class_names = dataset.classes\n","print(f'Class names: {class_names}')\n","\n","# train/validation split (70% train, 30% validation)\n","train_size = int(0.7 * len(dataset))\n","val_size = len(dataset) - train_size\n","trainset, valset = random_split(dataset, [train_size, val_size])\n","\n","# 확인: 데이터셋 크기 출력\n","print(f'Total images: {len(dataset)}')\n","print(f'Training images: {train_size}')\n","print(f'Validation images: {val_size}')\n"],"metadata":{"id":"Td6JmV6mRMjX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724386614369,"user_tz":-540,"elapsed":309,"user":{"displayName":"배고프다","userId":"10599614557067348512"}},"outputId":"2253d45c-d90c-462c-e881-88a71b22e237"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class names: ['alert', 'angry', 'frown', 'happy', 'relax']\n","Total images: 9325\n","Training images: 6527\n","Validation images: 2798\n"]}]},{"cell_type":"markdown","source":["# Training & Validation Define"],"metadata":{"id":"ywhsxwi6UERR"}},{"cell_type":"code","source":["def train(net, trainloader, optimizer, criterion, args):\n","\n","    net.train()\n","\n","    correct = 0\n","    total = 0\n","    train_loss = 0.0\n","\n","    for i, data in enumerate(trainloader, 0):\n","        optimizer.zero_grad()\n","\n","        # get the inputs\n","        inputs, labels = data\n","\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    train_loss = train_loss / len(trainloader)\n","    train_acc = 100 * correct / total\n","    return net, train_loss, train_acc"],"metadata":{"id":"Agz_cNDeUJEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(net, valloader, criterion, args):\n","\n","    net.eval()\n","\n","    correct = 0\n","    total = 0\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for data in valloader:\n","            images, labels = data\n","\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = net(images)\n","\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(valloader)\n","        val_acc = 100 * correct / total\n","    return val_loss, val_acc"],"metadata":{"id":"-KKQj6ZEeoEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(net, args):\n","\n","    testloader = torch.utils.data.DataLoader(testset,\n","                                              batch_size=args.test_batch_size,\n","                                              shuffle=False, num_workers=2)\n","    net.eval()\n","\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        test_acc = 100 * correct / total\n","    return test_acc"],"metadata":{"id":"1T9JRQymXGSf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment"],"metadata":{"id":"RQ9r6p_bW8y0"}},{"cell_type":"code","source":["def experiment(args):\n","\n","    # Dataloaders\n","    trainloader = torch.utils.data.DataLoader(trainset,\n","                                          batch_size=args.train_batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","    valloader = torch.utils.data.DataLoader(valset,\n","                                            batch_size=args.test_batch_size,\n","                                            shuffle=False, num_workers=2)\n","\n","    # Model Define (with pretrained weights)\n","    net = models.efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.DEFAULT)\n","\n","    \"\"\"\n","    EfficientNet의 self.classifier 구조\n","\n","     self.classifier = nn.Sequential(\n","        nn.Dropout(p=dropout, inplace=True),\n","        nn.Linear(lastconv_output_channels, num_classes),\n","    )\n","\n","    \"\"\"\n","    net.classifier[1] = nn.Linear(net.classifier[1].in_features, args.out_dim)\n","    if hasattr(args, 'dropout_rate'):\n","        net.classifier.add_module(\"dropout\", nn.Dropout(args.dropout_rate)) # dropout 사용할 경우 모델 아키텍처에 추가\n","\n","    net = net.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    if args.optim == 'SGD':\n","        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    elif args.optim == 'RMSprop':\n","        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    elif args.optim == 'Adam':\n","        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    else:\n","        raise ValueError('In-valid optimizer choice')\n","\n","    # 스케줄러 정의\n","    if args.scheduler == 'ReduceLROnPlateau':\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n","    elif args.scheduler == 'StepLR':\n","        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n","    elif args.scheduler == 'OneCycleLR':\n","        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(trainloader), epochs=args.epoch)\n","    elif args.scheduler == 'CosineAnnealingLR':\n","        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epoch)\n","    else:\n","        scheduler = None\n","\n","    train_losses = []\n","    val_losses = []\n","    train_accs = []\n","    val_accs = []\n","\n","    best_acc = 0.0\n","\n","    for epoch in range(args.epoch):  # loop over the dataset multiple times\n","        ts = time.time()\n","        net, train_loss, train_acc = train(net, trainloader, optimizer, criterion, args)\n","        val_loss, val_acc = validate(net, valloader, criterion, args)\n","        te = time.time()\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        train_accs.append(train_acc)\n","        val_accs.append(val_acc)\n","\n","        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch + 1, train_acc, val_acc, train_loss, val_loss, te-ts))\n","\n","        # 스케줄러 업데이트\n","        if scheduler is not None:\n","            if args.scheduler == 'ReduceLROnPlateau':\n","                scheduler.step(val_loss)\n","            else:\n","                scheduler.step()\n","\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            torch.save(net.state_dict(), 'your_best_model.pth')\n","            print(f'Best model saved with accuracy: {best_acc:2.2f}')\n","\n","    result = {}\n","    result['train_losses'] = train_losses\n","    result['val_losses'] = val_losses\n","    result['train_accs'] = train_accs\n","    result['val_accs'] = val_accs\n","    result['train_acc'] = train_acc\n","    result['val_acc'] = val_acc\n","    result['best_acc'] = best_acc\n","    return vars(args), result,net"],"metadata":{"id":"Gzi2dte1W-S4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Saving & Loading Experiment Results"],"metadata":{"id":"wpk40B1cAAOY"}},{"cell_type":"code","source":["import hashlib\n","import json\n","from os import listdir\n","from os.path import isfile, join\n","import pandas as pd\n","\n","def save_exp_result(setting, result):\n","    exp_name = setting['exp_name']\n","    # del setting['epoch']\n","    del setting['test_batch_size']\n","\n","    results_dir = f'{root_dir}/results'\n","    if not os.path.exists(results_dir):\n","        os.makedirs(results_dir)\n","\n","    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n","    filename = f'{root_dir}/results/{exp_name}-{hash_key}.json'\n","    result.update(setting)\n","    with open(filename, 'w') as f:\n","        json.dump(result, f)\n","\n","\n","def load_exp_result(exp_name):\n","    dir_path = './results'\n","    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n","    list_result = []\n","    for filename in filenames:\n","        if exp_name in filename:\n","            with open(join(dir_path, filename), 'r') as infile:\n","                results = json.load(infile)\n","                list_result.append(results)\n","    df = pd.DataFrame(list_result) # .drop(columns=[])\n","    return df"],"metadata":{"id":"MppM4rqPf6AZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Experiemt Parameters"],"metadata":{"id":"atHS_hjWWr_I"}},{"cell_type":"code","source":["# ====== Random Seed Initialization ====== #\n","seed = 123\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","args.exp_name = \"exp3_epoch\"\n","\n","# ====== Model ====== #\n","args.out_dim = 5\n","args.act = 'relu'\n","\n","# ====== Regularization ======= #\n","args.l2 = 0.00001\n","\n","# ====== Optimizer & Training ====== #\n","args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n","args.lr = 0.0001\n","args.lr_decay = 0.95\n","args.scheduler = 'CosineAnnealingLR'\n","#args.epoch = 10\n","\n","args.dropout_rate = 0.3\n","\n","args.train_batch_size = 64\n","args.test_batch_size = 128"],"metadata":{"id":"Sv_byRKlWyFg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import multiprocessing as mp\n","mp.set_start_method('spawn', force=True)\n","\n","# ====== Experiment Variable ====== #\n","name_var1 = 'lr'\n","name_var2 = 'epoch'\n","list_var1 = [0.0001]\n","list_var2 = [20, 30]\n","\n","\n","setattr(trainset, 'transform', None)\n","setattr(valset, 'transform', None)\n","# setattr(testset, 'transform', None)\n","\n","for var1 in list_var1:\n","    for var2 in list_var2:\n","        setattr(args, name_var1, var1)\n","        setattr(args, name_var2, var2)\n","        print(args)\n","\n","        setting, result,net = experiment(deepcopy(args))\n","        save_exp_result(setting, result)"],"metadata":{"id":"dAIUe_iKTmU4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba3774cd-e787-4c82-f189-0ea6ad824ab9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(exp_name='exp3_epoch', out_dim=5, act='relu', l2=1e-05, optim='RMSprop', lr=0.0001, lr_decay=0.95, scheduler='CosineAnnealingLR', dropout_rate=0.3, train_batch_size=64, test_batch_size=128, epoch=20)\n","Epoch 1, Acc(train/val): 52.57/70.16, Loss(train/val) 1.09/0.78. Took 247.46 sec\n","Best model saved with accuracy: 70.16\n","Epoch 2, Acc(train/val): 67.04/73.12, Loss(train/val) 0.76/0.70. Took 248.09 sec\n","Best model saved with accuracy: 73.12\n","Epoch 3, Acc(train/val): 75.47/72.37, Loss(train/val) 0.56/0.76. Took 246.92 sec\n","Epoch 4, Acc(train/val): 81.48/72.02, Loss(train/val) 0.41/0.84. Took 248.72 sec\n","Epoch 5, Acc(train/val): 82.96/72.12, Loss(train/val) 0.34/1.02. Took 248.61 sec\n","Epoch 6, Acc(train/val): 84.08/73.45, Loss(train/val) 0.30/1.10. Took 253.66 sec\n","Best model saved with accuracy: 73.45\n","Epoch 7, Acc(train/val): 84.65/71.91, Loss(train/val) 0.28/1.16. Took 253.21 sec\n","Epoch 8, Acc(train/val): 84.88/72.09, Loss(train/val) 0.27/1.19. Took 248.78 sec\n","Epoch 9, Acc(train/val): 85.49/72.73, Loss(train/val) 0.25/1.24. Took 249.28 sec\n","Epoch 10, Acc(train/val): 85.92/72.62, Loss(train/val) 0.25/1.30. Took 247.28 sec\n","Epoch 11, Acc(train/val): 85.78/72.09, Loss(train/val) 0.23/1.42. Took 250.69 sec\n","Epoch 12, Acc(train/val): 86.24/72.66, Loss(train/val) 0.22/1.52. Took 246.59 sec\n","Epoch 13, Acc(train/val): 86.79/72.91, Loss(train/val) 0.23/1.45. Took 250.77 sec\n"]}]},{"cell_type":"code","source":["# 학습 완료 후 your_best_model.pth 로드\n","net.load_state_dict(torch.load('your_best_model.pth'))\n","\n","# 테스트 실행\n","test_acc = test(net, args)\n","print(f'Test accuracy: {test_acc:.2f}%')"],"metadata":{"id":"jcQlFuDRW5Qu"},"execution_count":null,"outputs":[]}]}